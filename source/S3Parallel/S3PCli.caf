import &StandardImport, &Lib

allCommandOptions =
  quiet:                      "" no output
  verbose:                    "" extra output
  bucket:
    argument:     :bucket-name
    description:  "" The source bucket
    required:     true
  prefix:       :key          "" Only iterate over keys with this prefix. If 'startAfter' or 'stopAt' are also specified, the set-intersection of the two will be used.
  start-after:  :key          "" Start iteratating after this key. If 'prefix' is also specified, the set-intersection of the two will be used.
  stop-at:      :key          "" Iterate up to, and including, this key. If 'prefix' is also specified, the set-intersection of the two will be used.

  pattern:
    "" string OR js:/^any-javascript-regexp/i
    ""
      Source keys must contain the string, OR source keys must match the JavaScript regexp.
      Note: This won't speed up listing.
      Every key matching the specified prefifx, start-after and stop-at clause will ready from S3 and be tested with the provided pattern.
      It may, however, speed up copying by reducing the total number of files copied.

  filter:
    "" "js:({Key, Size, LastModified, ETag, StorageClass, Owner}) => true"
    ""
      Filter which items will be processed.
      Note: This won't speed up listing.
      Every key matching the specified prefifx, start-after and stop-at clause will ready from S3 and be tested with the provided filter.
      It may, however, speed up copying by reducing the total number of files copied.

writeOptions =
  dryrun:
    description: "" Will not modify anything. For sync/copy commands, do everything except actually copy files.
  pretend: description: "" alias for 'dryrun'

toBucketOptions =
  to-bucket:    argument: :bucket-name  required: true description:     "" The target bucket. It can be the same bucket.
  to-prefix:    :key-prefix         "" If 'prefix' is specified, the target key will REPLACE its source prefix with toPrefix Otherwise, this is the same as add-prefix.
  add-prefix:   :key-prefix         "" The source key is prepended with this string for the target bucket.
  to-key:       '"js:(key) => key"' "" Provide an arbitrary JavaScript function for re-keying keys.

toFolderOptions = # to-bucket copied here to REMOVE the required, since one
  to-bucket:    :bucket-name        "" The target bucket. It can be the same bucket. (to-bucket OR to-folder required)
  to-folder:    :to-folder          "" Path to a folder in the local file system. (to-bucket OR to-folder required)

advancedOptionsForAll =
  list-concurrency:        advanced: true argument: :100     description: ""Maximum number of simultaneous list operations
  max-list-requests:       advanced: true argument: :number  description: "" Not set by default; If set, will stop when hit. Use to limit how many requests get used.

advancedOptionsForCopy = merge advancedOptionsForAll,
  copy-concurrency:        advanced: true argument: :500       description: "" Maximum number of simultaneous small-copies
  large-copy-concurrency:  advanced: true argument: :75        description: "" Maximum number of simultaneous large-copies
  max-queue-size:          advanced: true argument: :50000     description: "" Maximum number of files that can be queued for copying before list-reading is throttled.
  large-copy-threshold:    advanced: true argument: :104857600 description:
    ""
      Files larger than this byte-size will use the large-copy strategy, which is currently a
      shell-exec of 'aws s3 cp'. Currently this must be set <= 5368709120 (5 gigabytes).
      This is s3.copyObject's max supported size, so S3P must shell-exec aws-cli for larger files.
      100 megabytes, the default, has been tested to be a good selection for maximum performance.


main: (options) ->
  &@art-suite/cli
  .start merge options,
    description:
      """
        S3 summarize, compare, copy, sync and more with massively parallel power.

        Configure AWS credentials with environment variables:
          s3p uses the same creds as the aws-cli. Learn more:
          https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html

        Source: https://github.com/generalui/s3p

    commands:
      each:
        run: &S3PCliCommands.each

        description:  "" Create your own iteration. Specify a --map or --map-list option.
        options:      merge allCommandOptions, advancedOptionsForAll,
                      map:        :function "" This gets called for each item found. A javascript function of the form (item) => ...
                      map-list:   :function "" This gets called with an array of items (length between 1 and 1000). A javascript function of the form (itemList) => ...
        examples: []
          "" each --bucket my-bucket --map "js:(item) => console.log(item)"
          "" Log every item found.

      list-buckets:
        run: &lib/S3.listBuckets
        description:  "" List all your S3 buckets.

      version:
        run: -> &package.version
        description:  "" Show s3p's version.

      summarize:
        run: &S3P.summarize
        description:  "" Scan all items in one bucket and produce a summary of all the items. Uses s3.listObjectsV2.
        options:      merge allCommandOptions,
                      summarize-folders: "" show count and size of each folder
                      advancedOptionsForAll
        examples:
          bucket: :my-bucket
          "" get a detailed summary of item counts and sizes in my-bucket

          bucket: :my-bucket filter: "" "js:({Size}) => Size > 1024*1024"
          "" summarize all files larger than 1 Megabyte

      map:
        run: &S3PCliCommands.map

        description: """ Map and reduce over the results of listBucket. Though 'map' and 'reduce' have default values, you'll likely want to override at least one of them. Further, you may wish to add a 'finally' function.
        options: merge allCommandOptions, advancedOptionsForAll,
          map: []
            :function
            """
              This gets called for each item found.
              Form: ({Key, Size, LastModified, ETag, StorageClass, Owner}) => ...
              Default: (a) => a
          reduce: []
            :function
            """
              Merge the two results of previous `map` or `reduce` calls into one.
              Form: (previousA, previousB) -> ...
              Default: (a, b) => require('art-standard-lib').compactFlatten([a, b])
          default: :any "" The default value to return if no items were found.
          finally: []
            :function
            ""
              If present, this function will be applied to produce the final result after the last call to reduce.
              Form: (finalReduceResult) -> ...

        examples:
          bucket: :my-bucket
          map:    "" "js:({Size}) => 1"
          reduce: "" "js:(a, b) => a + b"
          "" total count

          bucket: :my-bucket
          map:    "" "js:({Size}) => Size"
          reduce: "" "js:(a, b) => a + b"
          "" total file size

          bucket: :my-bucket
          reduce: "" "js:(a, b) => a.LastModified > b.lastModified ? a : b"
          "" newest item details

          bucket: :my-bucket
          reduce: "" "js:(a, b) => a.LastModified > b.lastModified ? a : b"
          finally: "" "js:({Key}) => Key"
          "" key of the newest item

      ls:
        run: &S3PCliCommands.ls
        description:  "" List all matching files. Uses s3.listObjectsV2.
        options:      merge allCommandOptions, advancedOptionsForAll, long: "" output item size and date as well as key

      compare:
        run: &S3P.compare
        description:  "" Compare two buckets and produce a summary of their differences. Uses s3.listObjectsV2.
        options:      merge allCommandOptions, toBucketOptions, advancedOptionsForAll
        examples:
          bucket: :my-bucket to-bucket: :my-to-bucket
          "" Compare items from my-bucket with my-to-bucket. Shows how many items exist in both, only one, or are difference sizes.

      cp:
        run: &S3P.copy
        description:
          """
            Copy all files from one bucket to another bucket. Uses s3.listObjectsV2, s3.copyObject and shell-exec 'aws s3 cp'.

            NOTE: This overwrites existing files in the target bucket. Try the 'sync' command for smarter copies when some of the files have already been copied.
        options:      merge allCommandOptions, toBucketOptions, writeOptions, advancedOptionsForCopy, toFolderOptions
        examples:
          bucket: :my-bucket to-bucket: :my-to-bucket
          "" Copy everything from my-bucket to my-to-bucket

          bucket: :my-bucket to-folder: :my/local/folder
          "" Copy everything from my-bucket to ./my/local/folder/*

          bucket: :my-bucket to-bucket: :my-to-bucket prefix: :2020-04-14/
          "" Copy everything from my-bucket to my-to-bucket with the prefix "2020-04-14/". The copied items will have the same keys as source items.

          bucket: :my-bucket to-bucket: :my-to-bucket prefix: :2020-04-14/ to-prefix: :2020-04-14-backup/
          "" Copy everything from my-bucket to my-to-bucket with the prefix "2020-04-14/" and REPLACES prefixes. Example: "2020-04-14/foo.jpg" is copied to "2020-04-14-backup/foo.jpg"

          bucket: :my-bucket to-bucket: :my-to-bucket prefix: :2020-04-14/ add-prefix: :backup/
          "" Copy everything from my-bucket to my-to-bucket with the prefix "2020-04-14/" and ADDS prefixes. Example: "2020-04-14/foo.jpg" is copied to "backup/2020-04-14/foo.jpg"

          bucket: :my-bucket to-bucket: :my-to-bucket prefix: :2020-04-14/ to-key: "" "js:(key) => key + '-old'"
          "" Copy everything from my-bucket to my-to-bucket with CUSTOM function that adds suffixes. Example: "2020-04-14/foo.jpg" is copied to "2020-04-14/foo.jpg-old"

      sync:
        run: &S3P.sync
        description:  "" Only copy files which do not exist in the target bucket. Uses s3.listObjectsV2, s3.copyObject and shell-exec 'aws s3 cp'.
        options:      merge allCommandOptions, toBucketOptions, writeOptions, advancedOptionsForCopy,
                      overwrite: "" If set, sync will overwrite existing files with different file sizes.

        examples:
          bucket: :my-bucket to-bucket: :my-to-bucket
          "" Copy everything from my-bucket to my-to-bucket

###
      """
        read-only commands:
          summarize   scan all items in one bucket and produce a summary of all the items (only uses s3-list)
          compare     compare two buckets and produce a summary of their differences      (only uses s3-list)
          list / ls   list all matching files

        write-commands:
          copy / cp   blindly copy all files from one bucket to another bucket
          sync        only copy files which do not exist in the target bucket

        options:
          all-commands:
            --bucket bucket-name
              The source bucket

            --prefix key
              Only iterate over keys with this prefix.

            --start-after key
              Start iteratating after this key
              If prefix and startAfter are specified, both will be enforced.

            --stop-at key
              Iterate up to, and including, this key
              If prefix and stopAt are specified, both will be enforced.

            --pattern string
              Source keys must contain this exact string

            --pattern "js:/^any-javascript-regexp/i"
              Source keys must match this JavaScript regexp.

            --filter "js:({Key, Size, LastModified, ETag, StorageClass, Owner}) => true"
              Filter results of listObjects.

            --quiet
              no output

            --verbose
              extra output

            --dryrun / --pretend
              Will not modify anything.
              For sync/copy commands, do everything except actually copy files.

          summarize-command
            --summarize-folders

          compare, copy, sync commands
            --to-bucket bucket-name
              The target bucket. Can be the same bucket.

            --to-prefix key-prefix
              if prefix is specified, the target key will REPLACE its source prefix with toPrefix
              Otherwise, this is the same as addPrefix.

            --add-prefix key-prefix
              The source key is prepended with this string for the target bucket.

            --to-key "js:(key) => key"
              Provide an arbitrary JavaScript function for re-keying keys.

          sync-only:
            --overwrite
              If set, sync will overwrite existing files with different file sizes.

          all-commands advanced:
            --list-concurrency        100
              Maximum number of simultaneous list operations

            --copy-concurrency        500
              Maximum number of simultaneous small-copies

            --large-copy-concurrency  75
              Maximum number of simultaneous large-copies

            --max-queue-size          50000
              Maximum number of files that can be queued for copying before list-reading is throttled.

            --large-copy-threshold    104857600
              Files larger than this byte-size will use the large-copy strategy, which is currently
              a shell-exec of 'aws s3 cp'.

            --max-list-requests       number
              Not set by default; If set, will stop when hit. Use to limit how many requests
              get used.

        examples:

          # get a detailed summary of item counts and sizes in my-bucket
          s3p summarize --bucket my-bucket

          # Compare items from my-bucket with my-to-bucket.
          # Shows how many items exist in both, only one, or are difference sizes.
          s3p compare --bucket my-bucket --to-bucket my-to-bucket

          # Copy everything from my-bucket to my-to-bucket
          s3p cp --bucket my-bucket --to-bucket my-to-bucket

          # Copy everything from my-bucket to my-to-bucket
          s3p sync --bucket my-bucket --to-bucket my-to-bucket

          # Copy everything from my-bucket to my-to-bucket with the prefix "2020-04-14/"
          # The copied items will have the same keys as source items.
          s3p cp --bucket my-bucket --to-bucket my-to-bucket --prefix 2020-04-14/

          # Copy everything from my-bucket to my-to-bucket with the prefix "2020-04-14/" AND:
          # REPLACES prefixes. Example: "2020-04-14/foo.jpg" is copied to "2020-04-14-backup/foo.jpg"
          s3p cp --bucket my-bucket --to-bucket my-to-bucket --prefix 2020-04-14/ --to-prefix 2020-04-14-backup/

          # copy everything from my-bucket to my-to-bucket with the prefix "2020-04-14/" AND:
          # ADDS prefixes. Example: "2020-04-14/foo.jpg" is copied to "backup/2020-04-14/foo.jpg"
          s3p cp --bucket my-bucket --to-bucket my-to-bucket --prefix 2020-04-14/ --add-prefix backup/

          # Copy everything from my-bucket to my-to-bucket with a custom key rewrite function
          # CUSTOM function adds suffixes. Example: "2020-04-14/foo.jpg" is copied to "2020-04-14/foo.jpg-old"
          s3p cp --bucket my-bucket --to-bucket my-to-bucket --prefix 2020-04-14/ --to-key "js:(key) => key + 'old'"

          # summarize all files larger than 1 Megabyte
          s3p summarize --bucket my-bucket --filter "js:({Size}) => Size > 1024*1024"
