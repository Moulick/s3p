import &StandardImport, {} &util.promisify, &awsS3MultipartCopy
exec = promisify &child_process.exec
shellEscape = &shellEscape
escape = (single) -> shellEscape [] single

&AwsSdk.config.setPromisesDependency &bluebird

class S3 extends BaseClass

  @awsSdkS3 = new &AwsSdk.S3 {} # useAccelerateEndpoint: true
  # https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html

  @classGetter
    s3: -> @awsSdkS3

  @list: ({bucket, prefix, limit=1000, startAfter}) =>
    startTime = currentSecond()
    @s3.listObjectsV2
      Bucket:     bucket
      Prefix:     prefix
      MaxKeys:    limit
      StartAfter: startAfter
    .promise()
    .tapCatch (error) ->
      log.error S3.list-error: {} bucket, prefix, startAfter, limit, error

    .then (results) ->
      duration = currentSecond() - startTime
      unless results.Contents is Array
        log.warn S3.list-no-Contents: {} bucket, prefix, startAfter, limit, duration, results
        throw new Error "S3.list: Contents is not an array"

      else if duration > 10
        log.warn S3.list-slow: {} bucket, prefix, startAfter, limit, duration, results: merge results, Contents: "Array #{results.Contents.length}"

      results.Contents

  ##
    TODO:
      For large files (>5GB), we need to just use the CLI:
        aws s3 cp s3://resbio-fastq-drop-internal/180316_NB501104_0354_AHM2L2BGX5/Undetermined/Undetermined_S0_L001_R1_001.fastq.gz s3://genui-dynamodb-migration-scratch/large-test.gz
        We can add --quiet (probably)
      Further, that should use acceleration, if enabled - still need to test that.
    IN:
      options:
        size:       Bytes (optional) if provided and if bigger than the max size allowed by copyObject, largeCopy is used
        bucket:     <String> (ignored if both from-bucket and to-bucket are provided)
        fromBucket: from-bucket (default: bucket)
        toBucket:   to-bucket (default: bucket)
        key:      <String>
        fromKey:  from-key (default: key)
        toKey:    to-key (default: key)
    OUT:
      Promise.then -> see https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#copyObject-property > Callback > Parameters
        {} CopyObjectResult, ...
  @copy: (options) =>
    @_normalizeCopyOptions(options) extract
      fromBucket, toBucket, fromKey, toKey, size
      pretend

    if pretend then Promise.resolve pretend: true
    else if size >= 1024**3
      # log largeCopy: {} fromBucket, toBucket, fromKey, toKey, size
      @largeCopy {} fromBucket, toBucket, fromKey, toKey

    else
      # log smallCopy: {} fromBucket, toBucket, fromKey, toKey, size
      @s3.copyObject
        CopySource: "" #{fromBucket}/#{fromKey}
        Bucket:     toBucket
        Key:        toKey
      .promise()

  # https://www.npmjs.com/package/aws-s3-multipart-copy
  # I think the configuration is right, but it fails with an invalid request error from AWS
  @_multipartCopy: ({fromBucket, toBucket, fromKey, toKey, size}) =>
    awsS3MultipartCopy.init
      @s3
      info:   (info) -> log multipart_info: info
      error:  (error) -> log.error multipart_error: error

    awsS3MultipartCopy.copyObjectMultipart
      source_bucket: fromBucket
      object_key: fromKey
      destination_bucket: toBucket
      copied_object_name: toKey
      object_size: size
      # copy_part_size_bytes: 50000000,
      # expiration_period: 100000

  @delete: (options) =>
    @s3.deleteObject
      Bucket: options.bucket
      Key:    options.key

  @largeCopy: (options) =>
    @_normalizeCopyOptions(options) extract fromBucket, toBucket, fromKey, toKey, pretend

    if pretend then Promise.resolve pretend: true
    else exec
      ""
        aws s3 cp
        #{} escape "" s3://#{fromBucket}/#{fromKey}
        #{} escape "" s3://#{toBucket}/#{toKey}

  # get object's metdata
  @headObject: ({bucket, key}) =>
    @s3.headObject
      Bucket: bucket
      Key: key
    .promise()

  @_normalizeCopyOptions: (options) ->
    options extract
      bucket, key
      fromBucket = bucket
      toBucket = bucket
      fromKey = key
      toKey = key


    unless present(fromBucket) && present(toBucket) && present(fromKey) && present toKey
      throw new Error "" Missing one of: fromBucket, toBucket, fromKey, toKey or bucket or key as a default
    merge options, {} fromBucket, fromKey, toBucket, toKey

  @shouldSyncObjects: (options) ->
    @_normalizeCopyOptions(options) extract fromBucket, toBucket, fromKey, toKey
    Promise.then ->
      if options.size < 1024**2 || !options.size
        true
      else
        @headObject options
        .then ({ContentLength}) -> ContentLength != options.size

  ## syncObject
    IN: options:
      passed to copyObject and shouldSyncObject
      size: used to compare and decide if it should copy
    OUT:
      copied: Bool
      status: if not copied, explains why
  @syncObject: (options) =>
    @shouldSyncObjects options
    .then (shouldSync) ->
      if shouldSync
        @copyObject options
        .then (result) -> merge result, copied: true

      else {} copied: false
        # status: "s3://#{toBucket}/#{toKey} exists and has same size (#{size}); skipping"
